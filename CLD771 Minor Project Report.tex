% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\author{}
\date{}

\begin{document}

\hypertarget{cld771-minor-project-report}{%
\section{\texorpdfstring{CLD771 Minor Project Report
}{CLD771 Minor Project Report }}\label{cld771-minor-project-report}}

Evaluation of B.Tech. Minor Project

Name: Samarth Bhatia Entry Number: 2019CH10124

Project Title: Using Deep Reinforcement Learning for scheduling gasoline
blending and distribution (SGBD)

Supervisor: Prof. Hariprasad Kodamana

 Report 1: 17 September, 2021

\textbf{Summary of work done:}

My supervisor, Prof. Kodamana told me to go through 2 papers (linked
below) and to solve the problem presented in Paper 1 using DRL methods,
as done in Paper 2.

My work so for was to

\begin{itemize}
\item
  thoroughly read the 2 papers
  \href{https://doi.org/10.1016/j.compchemeng.2019.106636}{Paper 1}
  \href{https://doi.org/10.1016/j.compchemeng.2020.106982}{Paper 2}.
\item
  Paper 1, "Scheduling of gasoline blending and distribution using
  graphical genetic algorithm", was about the problem of SGBD using a
  GGA. They consider certain variables and parameters associated with
  SGBD such as flow rates of products, costs of components, demand of
  orders, costs of changeovers of products in blenders and product
  tanks, and tardiness costs etc., which makes for a close-to-reality
  situation.
\item
  Paper 2, "A deep reinforcement learning approach for chemical
  production scheduling", applies Deep Reinforcement Learning to a
  production problem in Chemical Engineering in the form of a
  \textbf{A2C (Advantage Actor Critic) algorithm}, which involves 2
  neural networks, one for the actor (which decides what to do based on
  the current state), and one for the critic (which approximates a value
  function to objectively tell how good the actions taken by the actor
  are). \\
  This has recently been quite popular in DRL because the
  \textbf{Actor-Critic} model eliminates the disadvantages of using
  \textbf{Action-Value} methods (e.g. \textbf{Q-Networks} and
  \textbf{Deep QN}) like \emph{not being performant for continuous
  spaces}, and also of using \textbf{Policy-Gradient} methods (e.g. the
  \textbf{REINFORCE} algorithm) like \emph{only being able to update the
  policy after a whole episode of learning} since it is basically a
  Monte Carlo simulation (No Temporal Difference Learning).
\item
  I also learned a lot about Reinforcement Learning and Deep
  Reinforcement Learning from various lectures and an online course
  (Udacity DRL). Some of what I have learnt includes:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    interfacing with the OpenAI \texttt{gym} python package which is
    universally used in RL problems and also as a benchmark as it
    contains many standardized environments.
  \item
    The Bellman equation, a universal equation in RL applications:
  \end{enumerate}

  \[\underbrace{\text{New}Q(s,a)}_{\scriptstyle\text{New Q-Value}}=Q(s,a)+\mkern-34mu\underset{\text{New Q-Value}}{\underset{\Bigl|}{\alpha}}\mkern-30mu[\underbrace{R(s,a)}_{\scriptstyle\text{Reward}}+\mkern-30mu\underset{\text{Discount rate}}{\underset{\Biggl|}{\gamma}}*\mkern-75mu\overbrace{\max Q'(s',a')}^{\scriptstyle\substack{\text{Maximum predicted reward, given} \\ \text{new state and all possible actions}}}\mkern-45mu-Q(s,a)]\]

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    \textbf{Action-Value} method Q-Networks (and DQNs) and
    \href{https://github.com/Plutonium-239/deeprl-in-sgbd}{my
    implementations of them (GitHub)} on the
    \href{https://gym.openai.com/envs/MountainCar-v0/}{MountainCar-v0
    standard \texttt{gym}environment}
  \item
    \textbf{Policy-Gradient} method REINFORCE algorithm and
    \href{https://github.com/Plutonium-239/deeprl-in-sgbd}{my
    implementation of them (GitHub)} on the
    \href{https://gym.openai.com/envs/CartPole-v1/}{CartPole-v0 standard
    \texttt{gym} environment }
  \end{enumerate}
\item
  In addition to this, I have started work on the project part - I have
  started to implement the environment used in Paper 1, which will take
  some time to complete and will need to be changed as required (if the
  training implementation requires something differently) or because of
  optimizations in general. \\
  \emph{Note that this is the main part of the implementation describing
  the problem statement and every single constraint for proper results,
  and the training/agent part would not be very different from normal.
  However, I will have to change the agent slightly too to be compliant
  with the environment.}
\end{itemize}

\textbf{Future Plan:}

\begin{itemize}
\item
  In the immediate future, I will try and work out all the kinks of
  completing the environment. I also want this environment to be
  compatible with the \texttt{gym} package so that I would be able to
  release an open-source version of this SGBD problem (which will of
  course have less detail) as a python package for others to use.
\item
  Once the bulk of the environment is done, I will head to establishing
  some baseline results, considering the performance of the Graphical
  Genetic Algorithms in Paper 1, and also using prebuilt implementations
  like the \texttt{stable-baselines} package, the
  \texttt{catalyst}package (specialized for DRL using PyTorch for the NN
  part), etc.
\item
  After establishing baseline scores I will try different algorithms
  such as \textbf{A2C (Advantage Actor-Critic)}, \textbf{A3C
  (Asynchronous Advantage Actor-Critic)}, \textbf{PPO (Proximal Policy
  Optimization)} (which is a \textbf{Policy-Gradient} method) and others
  accordingly. I don't see much scoe for Deep Q-Networks for SGBD since
  most of the variables and actions are in the continuous space.
\end{itemize}

\end{document}
